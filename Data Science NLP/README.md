Fake News Classifier Using LSTM, RNN, Bidirectional RNN
This project is a significant endeavor in the realm of natural language processing and deep learning, driven by the urgency of tackling the pervasive issue of fake news in today's information landscape. In a world where the authenticity of news articles is often questioned, this project takes a data-driven approach to create a robust model capable of distinguishing between real and fake news content.
Project Objectives
* Develop and evaluate various deep learning models, including Simple Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM) networks, and Bidirectional LSTMs, to classify news articles as real or fake.
* Implement a comprehensive data preprocessing pipeline, including text cleaning, stemming, and one-hot encoding, to prepare the data for model training.
* Address the ethical considerations of fake news detection, including data bias, transparency, accountability, and privacy.
Workflow
1. Data Collection and Preprocessing
    * Collect a dataset of labeled news articles.
    * Preprocess the data using techniques such as text cleaning, stemming, and one-hot encoding.
2. Model Development
    * Develop and train different deep learning models, including RNN, LSTM, and Bidirectional LSTM models, for fake news classification.
3. Model Evaluation
    * Evaluate the performance of the models on a held-out test set.
4. Ethical Considerations
    * Discuss the ethical implications of fake news detection, including data bias, transparency, accountability, and privacy.
Getting Started
1. Clone this repository.
2. Install the required dependencies.
3. Run the Jupyter Notebook to train and evaluate the models.
Team Members
* Riddhi Vora (NUID: 002761911)
Dataset
The dataset used in this project is available from https://www.kaggle.com/c/fake-news/data.
Ethical Considerations
* Data Bias: The accuracy of the model is dependent on the quality and bias of the training data. It is important to be aware of potential biases in the dataset and to mitigate their impact on the model's performance.
* Transparency: The inner workings of the model should be transparent to the extent possible. This allows users to understand how the model makes decisions and to identify potential biases.
* Accountability: The developers and deployers of the model are accountable for its impact on society. It is important to use the model responsibly and to avoid unintended consequences.
* Privacy: The privacy of individuals should be protected when collecting and using data for fake news detection.

